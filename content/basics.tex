\chapter{Basics}

\section{Distribution Methods}
\label{distribution_basics}

\subsection{OpenMP and OpenACC}
OpenMP represents a compiler extension that allows programmers to harness parallel computational power on a machine. It adds functions and flags to the respective language (C, C++ and Fortran are currently supported), which indicate the execution environment how to parallelize the program \cite{openmp_spec}. The resulting code can then be run in multiple threads that can be scheduled on many CPU cores in parallel.

\begin{lstlisting}[language=C]
void example()
{
int i = 0;
int n = 10;
#pragma omp parallel for
for (i=1; i<n; i++) /* i is private by default */
b[i] = (a[i] + a[i-1]) / 2.0;
}

\end{lstlisting}


OpenACC offers similar functionality as OpenMP but aims at offloading parallel computations to attached accelerators like GPUs \cite{openacc_spec}. 

\subsection{MPI}
Although OpenMP and OpenACC are powerful tools for parallelization on a single machine, in order to write large scale software, communications across multiple machines are necessary. One standard tool to achieve this is MPI.
MPI offers bindings to many languages and extends such by functions to identify a process and send as well as receive messages.

\subsection{MapReduce}
Due to its simple programming model, which mainly consists of a \textit{Map} and a \textit{Reduce} phase it is a commonly used approach for large clusters. Its most prominent implementation is Hadoop MapReduce, which runs in combination with the Hadoop Distributed File System. It is therefore especially applicable for data intense jobs like log analysis.

\section{CUDA}

CUDA is an API released by NVIDIA, which enables NVIDIA GPUs to be used for general purpose computing. As modern GPUs are comprised of hundreds or even thousands of streaming processors, data parallel algorithms can make extensive use of the CUDA infrastructure.

\section{OpenCL}
\label{opencl}
OpenCL is a compute framework that represents a direct competitor to CUDA. It was initially proposed by Apple but is currently maintained by the Khronos Group, which consists of a multitude of companies like AMD, Intel, NVIDIA and others\cite{opencl_directors}. Unlike CUDA, OpenCL is not limited to NVIDIA GPUs but supports various device types and vendors. In fact OpenCL programs may be executed on CPUs, GPUs, FPGAs and digital signal processor (abbr. DSP). Prerequisite for the execution of OpenCL code on a targeted device is the presence of an OpenCL Installable Client Driver (abbr. ICD) on its enclosing host machine. Such ICDs must conform with the specification released by the Khronos Group. As of today, various implementations of OpenCL exist by vendors like AMD, NVIDIA, Intel, Apple, ARM, Xilinx and others \cite{opencl_conform}.

The most important definitions within the OpenCL specification are the introduction of its execution model as well as the underlying memory model, which in the following are explained based on the OpenCL Specification Version 2.2 document \cite{opencl_spec}.

\subsection*{Execution Model}
OpenCL programs are executed on selected OpenCL devices. An OpenCL program consists of at least on function that is called a Kernel. A Kernel is run for each work-item, which is the smallest partial unit of a submitted task. For instance a work-item could be an element of an array that is being handled. Kernels process the work-items using so called ranges, which define the size of the overall work-item space in up to three dimensions and allow the Kernel to determine the position of the work-item being processed. Ranges also define how the work-items are subdivided in work-groups. These relations are depicted in figure \ref{img:opencl_data_model} for a two dimensional array that is divided into work-groups in both dimensions.

\begin{figure}[!htb]	
	\includegraphics[width=0.55\textwidth]{drawings/opencl_data_model.pdf}
	\centering
	\caption{OpenCL Data Model}
	\label{img:opencl_data_model}
\end{figure}

In the OpenCL model devices consist of compute units (abbr. CU), which for instance are the cores a CPU. Each work-group is assigned to a CU that processes it. Through the execution model two layers of parallelization can be applied. Firstly work-groups are independent of each other which allows their parallel execution when a device has more than one CU installed. Secondly the underlying CUs may be able to parallelize the execution for work-items within a work-group as well. This is entirely depending on the respective hardware capabilities and vendor implementation. For example modern AMD GPUs execute work-groups in so called wavefronts. A wavefront can contain up to 64 work-items, which are executed in batches of 16 items per clock cycle and thus take 4 clock cycles to cover all enclosed items\cite{amd_opencl_spec}. This means that an optimal work-group size for these devices is a multiple of 64 to ensure full utilization of a wavefront.
NVIDIA GPUs on the other hand execute work-groups in so called warps, which can hold up to 32 work-items that are processed in parallel by 32 threads\cite{nvidia_pascal_spec}. This illustrates the level of abstraction of OpenCL, which enables to unify devices with significantly differing hardware implementations to compute under a single model.

Having many work-groups assigned per CU also enables OpenCL to hide latencies for required memory accesses. Whenever a CU needs to access memory, which would stall its threads, the CU may switch to another processable work-group and continue execution until this work-group also stalls. Thus the time for memory accesses is spent computing instead of waiting.

\subsection*{Memory Model}

Memory in OpenCL is divided into two major regions, the host memory and the device memory. The host memory is used by the host to store Kernels and execution data. For an execution the required data is transferred from the host memory to the device memory, which is subdivided into different levels:

\begin{description}[align=left,leftmargin=0cm]
  \item [Private] Private memory is only accessible by a single work-item that owns it. As such it is mainly used for temporary variables and allows no sharing to other work-items.
  \item [Local] Local memory is accessible by all work-items within a work-group. Thus it can be used to share and synchronize data within a group. For example intermediary results for an entire work-group can be produced.
  \item [Global] The global memory can be read by and written to by the host and all work-items. Therefore it serves to synchronize all work-groups. For instance the final results of a computation are stored in global memory to be read by the host.
  \item [Constant] Constant memory is similar to global memory but can only be allocated and initialized by the host. Work-items are only allowed to read data, which makes it meaningful for data that does not change throughout the computations.
\end{description}

All of the available memory levels have different sizes, depending on the utilized device and also require different clock cycles per access. For example, reading data from local memory can be a magnitude faster than from global memory but global memory offers far more space and allows for synchronization among all work-groups. Additionally calls to the global memory may be cached based on the caching capabilities of the respective device.

\subsection*{Kernels}

OpenCL Kernels are written in OpenCL C, which is a superset of C99, containing additional keywords that are related to memory access and synchronization. Kernels have to be started from so called host code, which runs on the machine that contains the targeted devices. The host code is usually written in C or C++ as the standard offers APIs for these languages. Still, third parties offer bindings for other languages. The following list contains some available language bindings:

\begin{description}[align=left,labelwidth=1.5cm]
  \item [Java] JOCL, Aparapi
  \item [.NET] OpenCL.NET, Cloo
  \item [Python] PyOpenCL, PyCL
  \item [Ruby] Ruby-OpenCL
\end{description}

The following kernel code describes the addition of two vectors, in which the elements at identical indexes of two arrays are accumulated:

\begin{lstlisting}[language=C++]
#pragma OPENCL EXTENSION cl_khr_fp64 : enable

__kernel void run(__global double *a, __global double *b, __global double *c){
  int i = get_global_id(0);
  c[i]  = a[i] + b[i];
}
\end{lstlisting}

Line 1 enables the OpenCL extension for double-precision floating point numbers, which is necessary for running the code. In line 3 the method declaration is visible that consists of three parameters. Each parameter is a pointer to an array of doubles residing in \textit{global} memory. As the Kernel is run for every work-item, line 4 identifies the global work-item index by calling \textit{get\_global\_id}. The acquired index is used in line 5 to process the actual computation where the result of the sum of array \textit{a} and array \textit{b} at the index is written to array \textit{c}.

For this example to run the host code has to choose an appropriate device, initiate data transfers and perform other auxiliary tasks. The host code is omitted for clarity as with around 50 lines of code it is considerably longer than the actual kernel code. 

While the OpenCL standard enables the execution of identical code on heterogeneous platforms, certain limitations still apply based on the capabilities of the utilized devices. For instance, code that uses the double data type can only be successfully run on devices that offer that feature. Executing the above snippet on an early 2015 Macbook Air may lead to two different results. On the one hand, execution on the built-in i5-5250U CPU will correctly add the two provided arrays and return the result. On the other hand the included Iris Graphics 6100 GPU will produce an error as it does not support the required OpenCL \textit{FP64} extension.

While OpenCL supports more device types and vendors it seems questionable whether having a general programming model comes with the price of performance penalties when compared to the specialized CUDA platform. Karimi, Dickson and Hamze have shown that CUDA code may be ported to OpenCL with only few modifications but will run between 16\% and 67\% slower depending on the problem size of their implemented Monte Carlo simulation\cite{performance_comparison}. On the opposite, Fang and Varbanescu and Sips have come to the conclusion that such performance gaps are influenced by the differences between the programming models and may be prevalent due to programming mistakes and compiler optimizations\cite{comprehensive_performance_comparison}. They reason that with all issues in mind, OpenCL can reach the same performance as CUDA.

\section{Aparapi}
\label{aparapi}
Aparapi is a library that offers extensive functionality to ease the usage of the OpenCL API and minimize programming effort when developing OpenCL Kernels. Firstly, it contains bindings to access OpenCL functions through a Java Native Interface (abbr. JNI), abstracting and bundling multiple low level calls to Java high level functions. Secondly, Aparapi is able to translate Java code to valid OpenCL kernels. Therefore, programmers can build classes that inherit the provided abstract Kernel class by Aparapi. The following code snippet shows such an implemented Kernel:

\begin{minipage}{\linewidth}
\begin{lstlisting}
final double[] a = new double[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
final double[] b = new double[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
final double[] c = new double[10];

Kernel kernel = new Kernel() {
  @Override
  public void run() {
     int i = getGlobalId();
     c[i] = a[i] + b[i];
  }
};

kernel.execute(10);

for(Double result:c){
  System.out.println(result);
}
\end{lstlisting}
\end{minipage}
The above code reproduces the vector addition example code from section \ref{opencl} in which elements at the same index of two arrays are summed and written to a third array. Ultimately, each element from the result array is printed. It also highlights that input data may be defined in Java code itself and the results are again available in Java after execution. This is possible as Aparapi automatically copies the participating data back and forth between the host code and the executing device. For a more granular approach, developers can also define, which data should be written or read in order to improve performance. This is especially useful when input data does not change during computation and therefore does not have to be copied back to the host. While an original OpenCL implementation would require roundabout 50 lines of auxiliary code, Aparapi abstracts tasks like device selection and data handling when no specific control is needed. This enables developers to program algorithms considerably faster and allows beginners quick access to OpenCL features without knowledge of low level mechanisms.

Although the code translation of Aparapi may work for many use cases, certain limitations apply to the Java code that can be handled. Information about these restrictions can be found in the documentation on GitHub \cite{aparapi_kernel_guidelines}, which is partly outdated as newer features lifted corresponding limitations. The following list represents a selection of Kernel restrictions that may affect common code patterns:

\begin{description}[style=nextline]
  \item [Data Types]
  The supported primitive data types are boolean, byte, short, int, long, and float. While Aparapi supports Java objects, it can only translate classes whose attributes are again primitives. Also such classes should allow access to the attributes through getter and setter functions.

  \item [Control Flow]
  Aparapi only supports conditionals and counting for-loops. Switch, break, continue, enhanced for-loops and recursion are not allowed.

  \item [Specific Device Choice]
  Although it can be specified on which device type a kernel shall run, it is not possible to select a specific device for execution. For example, on a machine with four different GPUs one could not decide through Aparapi, which of the four GPUs should be used.
\end{description}
