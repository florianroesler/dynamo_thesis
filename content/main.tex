\chapter{Dynamic OpenCL}

\section{Distribution Approach}
\label{distribution}
As portrayed in section \ref{distribution} and \ref{related} many viable options exist to distribute tasks among devices within a single machine as well as across a cluster. This section will explain the reasons for selecting the fitting approaches based on the goals declared in section \ref{goals}.

At first it is important to evaluate the options for running tasks on multiple devices within a single machine. While low level solutions like OpenMP or OpenACC are matured, they add considerable overhead to programming efforts. All communication and synchronization across multiple cores and devices has to be handled by the programmer, which adds introduces a layer of complexity through the utilization of directives. Thus, programmers have to be knowledgable and experiences in utilizing these directives for an efficient execution.

Instead OpenCL will be selected for the distribution of tasks within a single machine. While CUDA offers similar capabilities as OpenCL, it only supports NVIDIA GPUs and completely lacks CPU support. This contradicts with the proposed goal of a heterogenous environment in which devices of different types and vendors cooperate. OpenCL introduces a standardized form in which algorithms have to be designed following the defined memory model and work-item approach. Even though many synchronization and communication calls like data transfers are abstracted away from the programmer, significant low level knowledge is required for building algorithms in the framework. In order to create a more simplified approach, difficult issues for programmers should also be abstracted behind a meaningful API. The necessary steps for that are covered in section \ref{abstraction}.

Even though many solutions for distributing computational workloads among machines in a cluster, most require significant cluster management or programming efforts. Because OpenCL is selected as the underlying computational framework on the machines, the cluster distribution technology has to fit its capabilities.

Similar to OpenMP, MPI has matured over decades as a standard solution for low level communications between multiple machines. As such it has also been used frequently in conjunction with OpenCL and inspired frameworks to build upon this combination. Still, MPI remains a significant contributor to program complexity.

On the opposite MapReduce based frameworks like Hadoop MapReduce offer a strict programming model with a fixed API to follow for each implemented algorithm. As such it has also been used to run OpenCL on top of it within Map phases. In addition some solutions were created by researchers utilizing both technologies in conjunction. As Hadoop MapReduce requires HDFS as the mandatory file system, not only MapReduce nodes but also HDFS nodes have to managed within the cluster. Another restricting factor is the extensive usage of HDFS for writing intermediate results, which has negative impacts on performance.

While many of the previously described cluster distribution approaches are used in many professional projects, API forwarding libraries for OpenCL can offload computational workloads across a cluster without impacting programmers or introducing significant cluster management overhead. Available options with these capabilities include SnuCL, VirtualCL and dOpenCL as described in section \ref{related}. While all mentioned solutions have their own specialties, the key feature to be considered is the API forwarding, which has to work stable in order to ensure a well functioning distribution of tasks. Therefore as a first step the three frameworks were installed on the following cluster:

\begin{table}[htb]
  \centering
    \begin{adjustbox}{width=1\textwidth}
    \small
    \begin{tabular}{l | l | l | l}
    ~                     & Machine A                   & Machine B                  	& Machine C                  \\
    \hline
    CPU                   & Intel Xeon CPU E3-1284L v4 	& 4x Intel Xeon CPU E7-8890 v3 	& Intel Xeon CPU E3-1284L v4 \\
    RAM                   & 32GB                        & 128GB                       	& 32GB                       \\
    Interconnect          & 10 GBit/s                   & 1 GBit/s                  	& 10 GBit/s                  \\
    OS                    & Ubuntu 16.04.1 64 Bit       & Ubuntu 16.04.1 64 Bit      	& Ubuntu 16.04.1 64 Bit      \\
    OpenCL Driver Version & 1.2.0.25          			& 1.2.0.10002                   & 1.2.0.25                   \\
    \end{tabular}
    \end{adjustbox}

    \caption{Cluster Setup for Distribution Framework Tests}
    \label{table:cluster_setup_1}
\end{table}

As a first step, each of the frameworks was installed on the cluster following their respective included documentation. In the case of SnuCL the installation could not be completed using versions 1.3.2 and 1.3.3. While VirtualCL 1.24 could be installed without issues, executing various OpenCL programs led to inconsistent Segmentation Faults. The last candidate, dOpenCL 0.4.0r1819, was installed successfully and also managed to run the previously failed programs without issues. Therefore dOpenCL was chosen for more detailed benchmarks to investigate performance caveats.

Due to its architecture, dOpenCL has to communicate back and forth with remote devices over the network in order to send inputs and retrieve results. Thus, computations can be highly impacted by network transfers when the algorithm performs relatively quickly in comparison to the required input data. In the interest of creating such a data heavy algorithm, a matrix multiplication was implemented in OpenCL, which follows the naive school algorithm. This means that the complexity of the algorithm is $O(n^3)$. The only performance optimization that was undertaken is the transposition of the second matrix, which greatly improves cache efficiency.

In order to retrieve empirical facts about the performance depending on the network interconnection, matrix multiplications of different sizes were executed locally and remotely originating from Machine A to Machine B as well from Machine A to Machine C. To ensure that the network performance meets its specification, network performance tests were undertaken before benchmarking using iperf3 and ping. Both utilities were executed subsequently over the duration of 60 seconds with a measurement taken each second. The results are displayed in table \ref{table:cluster_interconnect_benchmarks}.

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{l | l | l}
			~                     & Machine B                  			& Machine C                  \\
			\hline
			iperf3                & 941.31 Mbit/s ($\sigma = 0.95$) 	& 9.409 Gbit/s ($\sigma = 0.051$) \\
			ping                  & 0.186 ms ($\sigma = 0.022$)  		& 0.14 ms ($\sigma = 0.012$)  \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{Cluster Interconnect Benchmarks}
	\label{table:cluster_interconnect_benchmarks}
\end{table}

From the benchmarks it can be inferred that both interconnects work close to their optimal performance concerning throughput and offer low latencies. Based on these results the matrix multiplication was executed. As $AB = C$, the two input matrices have to be transferred to the executing device and the resulting matrix has to be retrieved after completing the computation. This means that three data transfers have to be made, which in the case of two $8000x8000$ float matrices are $8000^2 * 4 bytes = 256 Megabytes$ per matrix. In order to distinguish the computational time from the data transfer times, data transfers were measured utilizing the OpenCL profiling options.

\begin{figure}[H]

\includegraphics[width=1.0\textwidth]{images/data_transfer.pdf}
\centering
\caption{dOpenCL Benchmark Remote vs. Local}
\label{img:data_transfer}
\end{figure}

It is visible from the obtained results that the local executions have minimal transfer times for the matrices as the only limiting factor is the bus interface between CPU and RAM. One the opposite, data transfers can have significant impact on performance for remote executions as the Ethernet transfers has to be added on top. This is especially true for fast machines that only have an inadequate interconnection speed like Machine B. While for matrix sizes of 8000x8000 the local execution only requires less than 2\% of the time for data transfers, its remote counterpart uses 25\% of the total. In the case of Machine C, the remote transfers profit from the 10 Gbit/s interconnect. As Machine C is considerably slower than Machine B during the computation phase, the fraction of data transfer time from the total is negligible.

Based on the results, one can conclude that the remote computation itself is nearly as fast as the local execution and that the introduced overhead by dOpenCL is marginal. Real performance degradations occur when data transfers have to be processed, which are directly affected by the network performance. Still, dOpenCL can greatly improve performance when accessing a superior machine via remote as seen in the example of Machine C and Machine B. In the given benchmark Machine A could profit from outsourcing the computation of 8000x8000 matrices to Machine B through a 5x speedup\footnote{Considering that Machine A and Machine C have identical setups and therefore assuming that both display equal performance.}.

\section{High-Level Abstraction}
\label{abstraction}

In section \ref{aparapi} it was shown how little code and knowledge of internals are required for building OpenCL computations with the help of Aparapi. Including the library in the eventual framework would not only benefit end users due to a simplified programming environment but also greatly assist during the creation of the framework itself. Through Aparapi Java could be used as the overall language of project while abandoning C++ for most parts with the exception of dOpenCL. Due to the choice of a more high level language for the less performance critical code, Java promises faster building time and better maintainability. In order to achieve this, Aparapi has to be connected to dOpenCL to harness the distribution capabilities. First, it has to be evaluated, whether Aparapi performs adequately compared to an original OpenCL algorithm. For that cause the matrix multiplication benchmark from section \ref{distribution} was transformed to Aparapi and executed on Machine C from the benchmark cluster. As Aparapi translates the Java code for Kernels it has not encountered before, the translation time would skew the results for the first iteration of a run. Therefore at first the translation time for differently sized Kernels was measured on Machine C with 100 iterations per Kernel:

\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{images/aparapi_translation.pdf}
	\centering
	\caption{Average Kernel translation time}
	\label{img:aparapi_translation}
\end{figure}

It is visible that the translation time of a kernel has a lower bound of ~30ms on Machine C. Additionally it can be concluded that the translation time is correlated to the Kernel size but even for many lines of code remains at around 350ms. In the light of long running computations this time becomes diminishable. Because the translation shall be ignored in the final results an initial run is executed to translate the Kernel before running the measured iterations. The matrix sizes as well as the number of iterations is identical to the benchmark from section \ref{distribution}.

\begin{figure}[H]
	\includegraphics[width=0.5\textwidth]{images/aparapivsopencl.pdf}
	\centering
	\caption{Aparapi vs. Handcrafted OpenCL}
	\label{img:aparapi_vs_opencl}
\end{figure}

As visible in the results, Aparapi can reach the performance level of handcrafted OpenCL code, which for the most runs only takes 1-2\% longer to compute. This minuscule difference could be the result of communication overhead through the JNI interface. Because of the similar performance it is valid to utilize Aparapi in the envisioned framework. As a first step the connection between Aparapi and dOpenCL should be evaluated. For this cause both components were installed in a development system and tested for interoperability. It was discovered that in their standard versions both pieces of software do not work in conjunction because of several bugs or design decisions. These problems and the corresponding solutions will be explained in the following part:

\begin{description}[style=nextline]
	\item [No available devices]
	When querying dOpenCL for appropriate devices from Aparapi, none would be returned even though multiple devices were available when using standard OpenCL. The problem for this symptom lies within dOpenCL, which insufficiently implements the query method. OpenCL uses a single byte to identify queried devices types. For instance \textit{00000010} queries CPUs and \textit{00000100} asks for GPUs. dOpenCL checks for the position of the bit and would correctly return devices of the requested task. The OpenCL standard also allows bitwise OR-operations on multiple types so that when requesting for CPUs and GPUs in one query: \textit{00000010 OR 00000100 = 00000110}. As Aparapi only supports CPUs and GPUs, it specifically asks for these devices even when the user only requests a CPU - filtering the CPUs is then done in Java. Because dOpenCL does not support these combined queries, it can not return any devices to Aparapi. Therefore this method was fixed, which resulted in Aparapi successfully obtaining information about available devices.
	
	\item [Specific device choice]
	Aparapi by design is focused on providing a simple API, which lacks certain features that are important for the framework. For example it is desirable to have multiple instances of the same Kernel run in parallel with different data. Therefore it would be possible to split tasks into smaller subsets of data that could be distributed to various devices. Aparapi allows to define preferences on which specific device a Kernel should be run. This preference is saved on class level instead of instance level, which means that all instances of the same class would be directed to the same device. Therefore no parallelization is possible with the standard Aparapi version. In order to fix this issue, the mechanism for memorizing preferences was modified to respect the definitions for each individual instance of a Kernel.
	
	\item [Failed compilations when using multiple devices]
	Due to fixing the previously described issues, the development system was able to successfully execute Kernels on a single node cluster. Adding a second machine with different installed devices produced an error during the OpenCL compilation process even when only one device was targeted for execution. In the given environment both nodes by itself were able to compile the Kernel without any incidents. Additionally it was verified that dOpenCL without Aparapi supported the multi node cluster. Therefore it was suspected that Aparapi had a bug that prevented the compilation when more than one machine was present. Through reviewing Aparapi internals the cause for the issue was found: Aparapi creates an OpenCL context not per device but per type. OpenCL contexts serve for sharing memory and control structures between multiple devices but can only be defined for devices of the same platform. Thus, when having multiple CPUs of different vendors in a cluster, Aparapi tries to create a context for all CPUs, which creates the error. In order to prevent this violation, in the fixed version contexts are created only for a single device.
	
\end{description} 
 

\section{Job Design}
\label{job_design}
With the implemented fixes described in section \ref{abstraction} Aparapi is able to successfully communicate with dOpenCL and deploy tasks to multiple remote devices. This ability enables jobs to be split up into subtasks, which can be computed in parallel. Therefore the envisioned framework has to provide a mechanism for submitting split tasks by the programmer. With such a mechanism the challenge arises how to distribute data among splits. While some problems may have no or little dependency across data, others may require complex interconnections between individual data points. Thus it is inevitable to ensure that each split has all the necessary data for its correct computation. On the opposite, naively including the full data set in each split may lead to serious performance penalties due to longer data transfer times. This becomes especially important with the introduction of remote execution libraries like dOpenCL, where the most likely bottleneck is the network connection.

Based on previous research the following methods were identified:

\begin{description}[style=nextline]
	\item [Manual Splits]
	Hadoop MapReduce enforces saving input data for the executable tasks in split files on the Hadoop Distributed File System. Each split file is executed independently and programmers have to define how to interpret a split during the map phase. Ultimately, the individually programmed reduce algorithm merges the splits together to form a final result.
	
	\item [Naive Buffer Replication]
	A simple and safe way to ensure that every split has the required data available is to transfer all data to the employed devices as done by \citeauthor{delalama_2012}\cite{delalama_2012}. Their method of buffer replication is trivial when transferring to the devices. On the opposite merging the individual output buffers back into the final result requires an algorithm to determine the parts that should be included by each split.
	
	\item [Intelligent Buffer Replication]
	\citeauthor{Kim_2011}\cite{Kim_2011} have developed a mechanism to automatically determine how to split input buffers among subtasks in OpenCL so that only minimal data has to be transferred. Their method is based on sampling memory access patterns before sending the data so that lower and upper memory bounds can be determined.
		
	\item [Meta Functions]
	\citeauthor{stepocl}\cite{stepocl} as well as \citeauthor{distcl}\cite{distcl} employ meta functions that are utilized to determine split data independent from the problem size. These meta functions have to be supplied by the programmer and define the memory access patterns by the algorithm.
	
\end{description} 

While all of the above mentioned methods are meaningful candidates, they each have negative impact either on performance or programming complexity: \textit{Naive Buffer Replication} increases total data transfer sizes, which enhances the chances of network congestion the higher the number of splits becomes. \textit{Intelligent Buffer Replication} requires an extra step for sampling memory accesses, which includes the translation of OpenCL to C code. Although \textit{Meta Functions} try to abstract efforts away from programmers they are still required to define the access patterns manually. \textit{Manual Splits} put the entire responsibility into the hands of the programmer who has to make sure that the data is split and merged back together. While it is the least automated method, it also grants full control over performance and is not dependent on split and merge algorithms that may fail for complex problems. Thus, the framework will support \textit{Manual Splits}. In order to show that such an approach is eligible in combination with Aparapi the following code example will illustrate the process of splitting the addition of two arrays into subtasks:

\begin{lstlisting}
public class AdditionKernel extends Kernel{
  int[] a, b, result;

  public AdditionKernel(int[] a, int[] b) {
    this.a = a;
    this.b = b;
    this.result = new int[a.length];
  }

  @Override
    public void run() {
    int i = getGlobalId();
    result[i] = a[i] + b[i];
  }
}

public class Addition {
  public static void main(String[] args) {
    final int tileCount = 2;
    int[] a = new int[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
    int[] b = new int[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
    int[] result = new int[a.length];

    int tileWidth = a.length / tileCount;
    for(int i = 0; i < tileCount; i++){
      int[] aTile = Arrays.copyOfRange(a, i * tileWidth, (i + 1) * tileWidth);
      int[] bTile = Arrays.copyOfRange(b, i * tileWidth, (i + 1) * tileWidth);

      AdditionKernel additionKernel = new AdditionKernel(aTile, bTile);
      additionKernel.execute(tileWidth);
      System.arraycopy(additionKernel.result, 0, result, i * tileWidth, tileWidth);
    }

    System.out.println(Arrays.toString(result));
  }
}

\end{lstlisting}

In the example a reusable Aparapi Kernel \textit{AdditionKernel} is defined that computes the sums of the two arrays independent of the general problem size. In the class \textit{Addition} the generated input data is split into smaller parts based on the \textit{tileCount}. Then for each tile the \textit{AdditionKernel} is instantiated and executed. After the execution the individual result is copied into the overall \textit{result} array. In order to grant higher parallelization the \textit{tileCount} could be dynamically changed into a divisor of the array length. This illustrates that writing a split and merge algorithm can be intuitive and not require much code. Additionally, other approaches like \textit{Meta Functions} would still require a substantial amount of code.

explain job/task relation and how Dynamo deals with it

iterative jobs

design considerations and limitations of Aparapi

\section{Hybrid Cloud}

In the previous sections benchmarks and code examples were shown based on the existence of a local cluster.
While local clusters may provide cheap and stable performance when completely utilized. In reality resource demands fluctuate, making it hard to reach an equilibrium with a full resource utilization and no job queue. For instance, the New York University published monthly utilization measurements for their three clusters in 2013\cite{nyu}, which are shown as a yearly aggregate in table \ref{table:cluster_utilization}. Aside from visible differences in average utilization all clusters show a considerably high standard deviation.

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{l | l | l | l}
			~						& Cluster 1	& Cluster 2	& Cluster 3                 \\
			\hline
			Average Utilization 	& 81.1\%  	& 71.0\% 	& 76.1\% \\
			Std. Deviation          & 5.2\%  	& 4.9\%		& 16.9\% \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{NYU Cluster Utilization}
	\label{table:cluster_utilization}
\end{table}

When providing a shared cluster that can be used by multiple parties, certain trade-offs apply. On the one hand overprovisioning decreases waiting times for users but increases costs and reduces utilization. On the other hand underprovisioning reduces costs and may lead to a 100\% utilization but comes with the price that users may have to line up in queues to access resources. Countering this trade-off comes the possibility to employ remote machines from cloud services, which can be booked and released dynamically and are billed hourly or even minutely. Among the most prominent services are Amazon's Elastic Compute Cloud (abbrv. EC2), Microsoft's Azure and Google's App Engine. While each cloud offer by the major players has its advantages and disadvantages, EC2 is the only that has bookable GPU instance types at various sizes. Although Azure and App Engine also have similar offerings, they are either only available as a preview in limited regions or can only be accessed by companies. Therefore, EC2 will be used as the exemplary cloud service for this research but with interchangeability in mind so that it is possible to replace EC2 with other services in the future.

Interconnecting the framework with EC2 requires including the AWS software development kit (abbrv. SDK), which is available for Java among many other languages. Through the SDK, the API for reserving and releasing instances can be accessed. Additionally, an Amazon Machine Image (abbrv. AMI) is required so that newly launched instances will have OpenCL installed and run the dOpenCL daemon by default on startup. For this cause an Ubuntu 14.04 based image was created with the appropriate drivers and dOpenCL installed, which is started right after booting the machine.

Ordering and releasing machines within the framework shall be handled by a class called \textit{Machine Managger}, abstract the actual implementation of the utilized SDK in order to enable users to interchange the underlying cloud provider:


\begin{lstlisting}
public abstract class MachineManager {
  protected abstract List<DynamoInstance> bookInstance(int count, String type);
  protected abstract void terminateInstances(List<DynamoInstance> instances);
  ...

  public List<DynamoInstance> book(int count, String type){
    List<DynamoInstance> instances = bookInstance(count, type);
    ...
    return instances;
  }

  public void release(List<DynamoInstance> instances){
    terminateInstances(instances);
    ...
  }
}
\end{lstlisting}

The above code shows the abstract class that handles bookings and releases of instances in the context of dOpenCL. In the actual implementation it includes functionalities which ensures that the booked instances are successfully booted and running the dOpenCL daemon before adding them to the cluster as well as removing them. These pieces have been deducted for the sake of readability. The actual implementation of the EC2 \textit{Machine Manager} that accesses the SDK is hidden behind the abstract class so that users only communicate with the generalized class. In order to employ another cloud provider, only the abstract methods have to be implemented that call the respective cloud API.

An important factor to consider when connecting a local cluster with a remote cloud is security. Remote machines should be sufficiently secured to prevent attackers from taking over machines or utilizing resources via calling the running dOpenCL daemon. By default the EC2 Ubuntu images only allow certificate based SSH, which prevents password attacks on any account. Additionally, Amazon offers the management of Security Groups that put machines behind a virtual firewall. Therefore the security group can be configured to only allow packages from the static IP range of the local cluster, which renders attacks from outside of the local network useless. Any newly booked instance will be included within the Security Group and as such automatically protected.

\section{Scalable Speed}

In section \ref{job_design} the general job design was explained that is employed in the framework. As jobs may contain an arbitrary number of tasks, this number determines the potential for parallelization. The limit exists as every task is only be executed on a single device. For example a matrix multiplication that has been split into four tasks may be computed by four devices in parallel and as such can benefit from a maximum four times speedup based on Amdahl's law. Choosing the appropriate number of tasks is influenced by a performance trade-off: while an increased number of tasks per job offers a greater parallelization and as such a potentially higher performance each task has to be scheduled by the framework and started in its own OpenCL Kernel. Thus, for certain jobs it may be beneficial to keep the number of tasks lower than the number of available devices in the cluster in order prevent performance penalties through overheads.

The general idea behind scaling performance within the framework is based on booking additional resources through a remote cloud when demand is high. Similarly, in the case of low demand resources can be released again in order to minimize costs. Widely employed cluster resource management solutions like Hadoop YARN include functionalities to add or remove nodes dynamically. In a YARN cluster each node is independent and communicates with a central master node that keeps track of utilized resources and handles each compute node as an individual entity. Offering a similar functionality in a dOpenCL based solution differs substantially:

OpenCL and Aparapi were built to run on a single machine and as such include assumptions of limited capabilities concerning changing hardware during operation. For example Aparapi caches device queries to OpenCL as devices are usually added or removed during maintenance procedures when a machine is powered down. dOpenCL overcomes such limitations and enables the central machine to have devices virtually installed by adding nodes to the dOpenCL cluster. dOpenCL therefore extends the OpenCL standard by adding the custom methods \textit{clCreateComputeNodeWWU} and \textit{clReleaseComputeNodeWWU}. 

As OpenCL itself has no multi-machine capabilties it only offers information for the respective built-in devices through the standard API. Therefore all devices appear as if they were installed in the central machine that runs the dOpenCL library. This makes identifying the owning machine per device impossible. Without the relation between devices and machines it can not be ensured that a machine meant to be released is not running any tasks. In order to provide the machine-device relation dOpenCL introduced another method called \textit{clGetDeviceIDsFromComputeNodeWWU}.

The mentioned dOpenCL methods are available to C++ programmers when including the respective header files that contain the extensions. Thus, host code that wants to benefit from dynamic device management has to be modified appropriately. In its standard version Aparapi is bound to the OpenCL standard and has no understanding of the offered dOpenCL extensions. Therefore it is necessary to modify the Aparapi JNI to enable dynamic resource scaling. In order to do so, two new JNI methods are introduced, \textit{addNode} and \textit{removeNode}. Both call the respective dOpenCL functions, with \textit{addNode} also reporting back the available devices of the added node.

introduce general scaling idea based on the job design

show the necessary changes towards dopencl and aparapi

\section{Optimized Scheduling}



explain general scheduling structure of dynamo

bring out the fancy stuff like performance history based approach and based on location of the machine

how about simplex?

\section{Exemplary Use Cases}



pure shared local cloud without scaling options

pure remote cloud with the server running in the cloud itself

hybrid cloud

use the framework as a library for a job itself
\section{Limitations}

show bottlenecks (memory of central machine and network in total)

jobs that make sense

job design limitations

