\chapter{Benchmarking Methodology}
\label{benchmarking_methodology}
\section{Measurements}

Throughout this paper various benchmarks are used to prove the feasibility of certain technologies and approaches. In all benchmarks the overall execution time of a tested task is measured. This metric recorded by using the wall clock times reported by the respective utilized programming language. In order to guarantee valuable measurements the impact of measurement errors has to be kept minimal. For instance it has to be ensured that the various clocks used for measuring the execution times provide satisfying precision. For measurements of OpenCL code, the built-in OpenCL profiler is used by calling \textit{clGetEventProfilingInfo}. It provides nanosecond granularity but the actual precision depends on the employed device\cite{cl_profiling}, which can be obtained by reading a device attribute through OpenCL. On all tested devices this value is determined to be below a millisecond. Timings that are taken in C++ are determined by the \textit{high\_resolution\_clock}, which also allows measurements with nanosecond precision on the tested systems. In the case of runtime measurements in Java, the command \textit{System.currentTimeMillis()} is used, which has a resolution depending on the underlying OS\cite{oracle_system}. On Ubuntu 16.04.1, which is employed in all benchmarks, the Java clock increases in steps of a single millisecond. Thus all clocks provide a precision of a millisecond or less. As the tested workloads are chosen to run at least multiple seconds and in many cases will take minutes to complete, this is deemed to be sufficient.

In order to reduce any virtualization overhead, local systems run their OS directly on bare metal hardware. In the case of cloud resources this is not possible because of the necessary virtualization infrastructure to support multi tenancy.

For all systems it is ensured that there are no other users using it. Additionally, the number of background processes that could significantly utilize computational power are shutdown before testing. This ensures that no significant portion of performance is lost to processes that are not relevant to the corresponding benchmark.

In most test cases the available network speed plays a crucial role to performance. Therefore the networking capabilities between two interacting systems are measured before testing to identify possible shortcomings. The recorded metrics deemed relevant are bandwidth and latency, which are supplied by the command line tools iperf3 and ping. Both tools are run sequentially for at least a minute with a sample taken each second.

Dynamic OpenCL is written in Java, which means that most benchmarks will employ the JVM for parts of the application. Measuring performance for algorithms that run in the JVM usually requires extended control over some non-deterministic features, namely just-in-time compilation and garbage collection\cite{rigorous_java}. For Dynamic OpenCL the JVM is only used for management code while the actual computations are executed in OpenCL. Therefore these non-deterministic effects are neglected as the Java code only takes up small fractions of the computational capabilities of a machine.

For all recorded benchmarks multiple samples are taken. As most workloads are tested in various problem sizes, at least five measurements are taken per size. For short running single test cases at least 30 samples are taken. All results are provided including the arithmetic mean with corresponding standard deviation. In the case of multiple sequential executions of the same workload an initial warm up run is executed that is not recorded in the results. Instead it serves to initialize certain system components that would otherwise skew the run time of the first execution.

\section{Cluster Composition}
\label{cluster_composition}
Throughout the research various benchmarks are presented to highlight caveats and performance differences of the showcased solutions. For this reason a standard set of hardware shall be used that remains constant across various benchmarks. The following table displays the machine classes that are utilized throughout the work:

\begin{table}[htb]
	\centering
	\begin{adjustbox}{width=0.7\textwidth}
		\small
		\begin{tabular}{l | l | l | l}
			~                     & Class A                  	& Class B                  \\
			\hline
			CPU                   &  4x Intel Xeon CPU E7-8890 v3 	& Intel Xeon CPU E3-1284L v4 \\
			Logical Cores         &  144 	& 8 \\
			RAM                   &  128GB                       	& 32GB                       \\
			Ethernet	          &  1 GBit/s                  	& 10 GBit/s                  \\
			OS                    &  Ubuntu 16.04.1 64 Bit      	& Ubuntu 16.04.1 64 Bit      \\
			OpenCL Driver Version &  1.2.0.10002                   & 1.2.0.25                   \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{Cluster Composition}
	\label{table:cluster_setup_1}
\end{table}

All of the machines belonging to the above classes are installed in the same local network and connected via the specified interconnects. In order to retrieve empirical facts about the performance depending on the network interconnection, the real network performance of the machines has to be measured. Therefore tests were undertaken from a machine within the same network using iperf3 and ping. Both utilities are executed subsequently over the duration of 60 seconds with a measurement taken each second. The results are displayed in table \ref{table:cluster_interconnect_benchmarks}. It can be inferred that both interconnects work close to their optimal performance concerning throughput and offer low latencies.

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{l | l | l}
			~                     & Class A                 			& Class B                  \\
			\hline
			iperf3                & 941.31 Mbit/s ($\sigma = 0.95$) 	& 9.409 Gbit/s ($\sigma = 0.051$) \\
			ping                  & 0.186 ms ($\sigma = 0.022$)  		& 0.14 ms ($\sigma = 0.012$)  \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{Cluster Network Benchmarks}
	\label{table:cluster_interconnect_benchmarks}
\end{table}

Some of the benchmarks employ cloud instances provided by Amazon's EC2. Customers may choose machines from a range of instance types to fit their specific needs. For the test cases in this research two computational high performance types are chosen, one CPU type and one GPU type. The following table shows the details for the respective instance types:

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.65\textwidth}
		\small
		\begin{tabular}{l | l | l | l}
			~                     & c4.8xlarge                	& g2.2xlarge                 \\
			\hline
			CPU                   &  Intel Xeon E5-2666 v3 	& Intel Xeon E5-2670 \\
			Logical Cores         &  36 	& 8 \\
			GPU                   &  -						& GRID K520 \\
			RAM                   &  60GB                       	& 15GB                       \\
			Ethernet          &  10 GBit/s                  	& 1 GBit/s                  \\
			OS                    &  Ubuntu 14.04.4 64 Bit      	& Ubuntu 14.04.4 64 Bit      \\
			OpenCL Driver Version &  1.2.0.25                   & 1.2 CUDA 367.57        \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{EC2 Cluster Composition}
	\label{table:cluster_setup_ec2}
\end{table}

For both instance types the network speeds are measured by launching the iperf3 and ping from within the EC2 on a third c4.8xlarge instance. The results can be seen in table \ref{table:cluster_interconnect_benchmarks} and demonstrate very high bandwidths and low latencies.

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{l | l | l}
			~                     & c4.8xlarge               			& g2.2xlarge                \\
			\hline
			iperf3                & 9.44 Gbit/s ($\sigma = 0.068$) 	& 992.6 MBit/s ($\sigma = 4.8$) \\
			ping                  & 0.158 ms ($\sigma = 0.021$)  		& 1.29 ms ($\sigma = 0.047$)  \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{EC2 Network Benchmarks}
	\label{table:cluster_interconnect_benchmarks}
\end{table}


\section{Workloads}
\label{workload_explanation}
In this section the various utilized algorithms that are employed in the respective benchmarking scenarios are analyzed. On the one hand it is necessary to understand the complexity class of a computation for a suitable interpretation of the results. On the other hand the amount of data transfers must be analyzed as well as strategies for the separation of data in order to distribute partial tasks to multiple machines.

\subsection*{Matrix Multiplication}
\label{matrix_multiplication_workload}
In its naive algorithm the multiplication two square matrices of size $n\times n$, for every element in the resulting matrix of size $n\times n$, $n$ multiplications have to be performed. Therefore the computation belongs to the complexity class $O(n^3)$. Still, there exist much more sophisticated algorithms improving the complexity class like Strassen's algorithm that reduces the complexity to $O(n^{2.807})$\cite{strassen}. An even further improvement for large matrices represents the approach by Coppersmith with $O(n^{2.376})$\cite{coppersmith}. For the sake of simplicity throughout this research the naive algorithm is used. This also grants relatively long execution times in relation to the input and output data sizes.

It must be noted that a matrix multiplication can not be perfectly split for parallelization. Instead, one of the matrices has to be present in its entirety, while the second matrix can be divided. This means that there is a growing overhead correlating to the amount of splits. With every additional split the input data is increased by 50\%. Therefore while an increased parallelization may grant faster computational capabilities of a cluster it also requires more data transfers.

\subsection*{Mandelbrot Set}

Mandelbrot sets can be used to draw fractals by calculating membership to the set for each coordinate of an image. Assuming a square image the computation belongs to the complexity class $O(n^2)$. An important feature of Mandelbrot calculations is the possibility to increase computation times without growing the size of the image. This can be done by raising the iterations factor, which determines the precision of the resulting picture. Therefore Mandelbrot set computations can be very time consuming while requiring only small input data.

In terms of parallelization Mandelbrot sets can be perfectly split as only the coordinates of a point are required to compute membership. Thus, there is no data overhead involved in dividing the computation to multiple machines.

\subsection*{N-Body Problem}

The n-body problem is mainly used in astrophysics to determine celestial movement, which is heavily influenced by gravitational effects. Celestial bodies act forces on each other, even when distances are huge. For instance, the earth is influenced by every existing object in the universe but most of these forces are very weak in comparison to the sun's gravitation. Calculating the n-body problem without approximations therefore requires to accumulate the imposed forces on every object by all other objects. Thus, the complexity class is $O(n^2)$.

It must be noted that dividing the n-body problem for parallelization leads to significant overhead as each split requires the whole dataset to calculate influences by every object. This means that for $n$ splits the data set has to be transfered $n$ times as well.

\subsection*{K-Means Clustering}

K-means clustering is an iterative data mining algorithm used for assigning $n$ objects to $k$ clusters. In order to determine membership of an object, its distance (e.g. Euclidean distance) to each of the $k$ clusters is determined, which reveals the nearest cluster. This cluster then inhibits the object. For each iteration the cluster coordinates are reassigned to their respective centroids and the membership of each object is recalculated. This is repeated for a fixed number of times or until a specified accuracy is reached, which indicates that the centroids converge.

With $n$ objects, $k$ clusters, $d$ dimensions and $i$ iterations the complexity of k-means clustering becomes $O(n*k*d*i)$. In order to simplify the complexity during benchmarking, all variables but the number of objects remain fixed, thus leading to a complexity of $O(n)$.

The computation can be parallelized per iteration where each split receives its partial set of objects as well as all cluster coordinates. Therefore the only duplication happens for the cluster centroids, which in practice are of significantly fewer than the number of objects.
