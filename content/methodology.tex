\chapter{Benchmarking Methodology}
\label{benchmarking_methodology}
\section{Measurements}

The metric that is taken for all performance measurements is the wall clock time provided by the respective utilized programming language. For measurements of OpenCL code the builtin OpenCL profiler by calling \textit{clGetEventProfilingInfo} is used, which provides nano-second granularity. Timings that are taken in C++ are determined by the \textit{high\_resolution\_clock}. In the case of runtime measurements in Java, the command \textit{System.currentTimeMillis()} is used, which has a resolution depending on the underlying OS but is usually around tens of milliseconds. As the benchmarkes workloads are chosen to run at least multiple seconds and in many cases will take minutes to complete, the given precisions of the timings are deemed to be sufficient.


taken on bare metal machines
no other users on the system
make sure that unnecessary processes are killed
for network based tests measure real network speed beforehand
network speed can not be measured while task is running
talk about JVM specifics
always report the arithmetic mean and standard deviation
in case of short running measurements >30 runs
in case of long running measurements with different problem sizes >5 runs per size but with >30 runs across all problem sizes

\section{Cluster Composition}
\label{cluster_composition}
Throughout the research various benchmarks are presented to highlight caveats and performance differences of the showcased solutions. For this reason a standard set of hardware shall be used that remains constant across various benchmarks. The following table displays the machine classes that are utilized throughout the work:

\begin{table}[htb]
	\centering
	\begin{adjustbox}{width=1\textwidth}
		\small
		\begin{tabular}{l | l | l | l}
			~                     & Class A                  	& Class B                  \\
			\hline
			CPU                   &  4x Intel Xeon CPU E7-8890 v3 	& Intel Xeon CPU E3-1284L v4 \\
			Logical Cores         &  144 	& 8 \\
			RAM                   &  128GB                       	& 32GB                       \\
			Interconnect          &  1 GBit/s                  	& 10 GBit/s                  \\
			OS                    &  Ubuntu 16.04.1 64 Bit      	& Ubuntu 16.04.1 64 Bit      \\
			OpenCL Driver Version &  1.2.0.10002                   & 1.2.0.25                   \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{Cluster Composition}
	\label{table:cluster_setup_1}
\end{table}

All of the machines belonging to the above classes are installed in the same local network and connected via the specified interconnects. In order to retrieve empirical facts about the performance depending on the network interconnection, the real network performance of the machines has to be measured. Therefore tests were undertaken from a machine within the same network using iperf3 and ping. Both utilities were executed subsequently over the duration of 60 seconds with a measurement taken each second. The results are displayed in table \ref{table:cluster_interconnect_benchmarks}. It can be inferred that both interconnects work close to their optimal performance concerning throughput and offer low latencies.

\begin{table}[!htb]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\small
		\begin{tabular}{l | l | l}
			~                     & Machine A                 			& Machine B                  \\
			\hline
			iperf3                & 941.31 Mbit/s ($\sigma = 0.95$) 	& 9.409 Gbit/s ($\sigma = 0.051$) \\
			ping                  & 0.186 ms ($\sigma = 0.022$)  		& 0.14 ms ($\sigma = 0.012$)  \\
		\end{tabular}
	\end{adjustbox}
	
	\caption{Cluster Interconnect Benchmarks}
	\label{table:cluster_interconnect_benchmarks}
\end{table}

\section{Workloads}
\label{workload_explanation}
In this section the various utilized algorithms that are employed in the respective benchmarking scenarios will be analyzed. On the one hand it is necessary to understand the complexity class of a computation for a suitable interpretation of the results. On the other hand the amount of data transfers must be analyzed as well as strategies for the separation of data in order to distribute tasks to multiple machines.

\subsection*{Matrix Multiplication}
\label{matrix_multiplication_workload}
In its naive algorithm to multiply two square matrices of size $n\times n$ with each other, for every element in the resulting matrix of size $n\times n$, $n$ multiplications have to be performed. Therefore the computation belongs to the complexity class $O(n^3)$. Still, there exist much more sophisticated algorithms improving the complexity class like Strassen's algorithm with $O(n^{2.807})$\cite{strassen} or the approach by Coppersmith with $O(n^{2.376})$\cite{coppersmith}. For the sake of simplicity, in all benchmarks the naive algorithm will be used.

It must be noted that a matrix multiplication can not be perfectly split for parallelization. Instead, one of the matrices has to be present as a whole, while the other can be divided. This means that there is a growing overhead correlating to the amount of splits. The size of the data transfer for the input is calculated as follows for two square matrices that have $n$ columns, $n$ rows and therefore are comprised of $n^2$ elements:

$s$ is the number of even splits with $0 < s <= n$.

Therefore the number of transferable elements is
$e = s*(\frac{n^2}{s}+n^2) = n^2 + s*n^2$.

For instance, two 16x16 matrices that are multiplied will yield the following input data sizes depending on the number of splits:

When $s=1$: $16^2+1*16^2 = 512$

When $s=2$: $16^2+2*16^2 = 768$

When $s=4$: $16^2 + 4*16^2 = 1280$

This means that dividing the data results in a 50\% increase of input data for each additional split. Therefore while an increased parallelization may grant faster computational capabilities of a cluster it also requires more data transfers.

\subsection*{Mandelbrot Set}

Mandelbrot sets can be used to draw fractals by calculating membership to the set for each coordinate of an image. Assuming a square image the computation belongs to the complexity class $O(n^2)$. An important feature of Mandelbrot calculations is the possibility to increase computation times without growing the size of the image. This can be done by raising the iterations factor, which determines the precision of the resulting picture. Therefore Mandelbrot set computations can be very time consuming while requiring only small input data.

In terms of parallelization Mandelbrot sets can be perfectly split as only the coordinates of a point are required to compute membership. Thus, there is no data overhead involved in dividing the computation to multiple machines.

\subsection*{N-Body Problem}

The n-body problem is mainly used in astrophysics to determine celestial movement, which is heavily influenced by gravitational effects. Celestial bodies act forces on each other, even when distances are huge. For instance, the earth is influenced by every existing object in the universe but most of these forces are very weak in comparison to the sun's gravitation. Calculating the n-body problem without approximations therefore requires to accumulate forces by every object for every other celestial body. Thus, the complexity class is $O(n^2)$.

It must be noted that dividing the n-body problem for parallelization leads to significant overhead as each split requires the whole dataset to calculate influences by every object. This means that for $n$ splits the data set has to be transfered $n$ times as well.

\subsection*{K-Means Clustering}

K-means clustering is an iterative data mining algorithm used for assigning $n$ objects to $k$ clusters. In order to determine membership of an object, its distance (e.g. Euclidean distance) to each of the $k$ clusters is determined, which reveals the nearest cluster. This cluster then inhibits the object. For each iteration the cluster coordinates are reassigned to their respective centroids and the membership of each object is recalculated. This is repeated for a fixed number of times or until a certain accuracy is reached in which the centroids converge.

With $n$ objects, $k$ clusters, $d$ dimensions and $i$ iterations the complexity of k-means clustering becomes $O(n*k*d*i)$. In order to simplify this, all variables but the number of objects will be fixed, thus leading to a complexity of $O(n)$.

The computation can be parallelized per iteration where each split receives its partial set of objects as well as all cluster coordinates. Therefore the only duplicated data are the centroids of the clusters, which in practice are of significantly smaller number than objects.
