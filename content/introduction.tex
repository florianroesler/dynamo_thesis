\chapter{Introduction}

\section{Motivation}

At the current state of technology, computers with multiple compute processing units (abbr. CPU) cores are common and even small devices like smartphones are usually manufactured with a multi-core architecture. Programs that want to harness the full potential of a multi-core computer have to be constructed accordingly.

Additionally, certain use cases require more computational power than a single machine can provide. Therefore it is mandatory to connect multiple machines to collectively solve a problem. For example researchers recently created a method to break a widely used cryptographic hash function that requires resources equivalent to more than 6500 CPU years to compute \cite{shattered}. A single machine at the current level of technology would therefore require too long in order to deliver the desired results. At the same time certain physical factors are limiting further quick advancements concerning CPU speeds \cite{end_of_moores_law}\cite{end_of_silicon} so that many problems will still require multiple machines in the future for time efficient computations.

As of today, there are various available approaches for programmers to distribute workloads across multiple cores of a single machine or among multiple machines grouped as a cluster. Enhancing single threaded code to a multithreaded program that runs on multiple machines in parallel adds several layers of complexity to a project, which may introduce errors and increased maintenance efforts.

Frameworks like Open Multi-Processing (abbr. OpenMP), CUDA or Open Computing Language (abbr. OpenCL) allow for the parallel utilization of resources within a machine but in return require programmers to learn extensive APIs or write code in low level languages like C.
At the same time cluster distribution approaches like Hadoop MapReduce require extended configuration management of the participating nodes. MapReduce as well as other techniques like the Message Passing Interface (abbr. MPI) also add a layer of complexity to the written program code that is entirely focused on synchronizing the distributed parts.
Ultimately, the combination of both levels of parallelization may lead to additional problems, which distract programmers from focusing on the underlying algorithm.

While the creation of distributed algorithms implicates various problems on its own, running these programs in a cost efficient manner represents another elaborate task. Clusters dedicated to running computations in a shared environment should have as little underutilization as possible in order to minimize costs. Therefore the dynamic adaption of resources to the actual performance requirements is a challenge with significant monetary benefits. As a result, cloud services like Amazon Elastic Compute Cloud (abbr. EC2) have seen a steady increase in revenue due to their flexible billing options that allow booking resources for short amounts of time\cite{gartner_2017}. Still, a pure cloud approach may introduce several disadvantages like higher costs or reduced performance due to geographical distances between the user and the utilized resources. Accordingly, a hybrid approach appears beneficial, providing a steady baseline of performance with local hardware. In the case of computational peaks additional resources might be booked from an external provider, which allows for lower costs and better quality of service during times with high resource demands.

While there are existing solutions to run distributed code most usable frameworks do not offer programmers to distribute their workloads among varying cluster configurations at a high abstraction level. Therefore the goal of this research is to create a framework that combines present partial solutions and provides additional functionality.

\section{Goals}
\label{goals}
The main goal of this research is to provide a framework that assists programmers to distribute parallel programs within a cluster. Thus, the following requirements are defined in order to streamline the development:

\begin{description}[style=nextline]
    \item [Heterogeneity]
    The framework should support modern CPUs and GPUs. Thus, devices by the following vendors should be included: AMD, ARM, Intel and NVIDIA. Therefore various workloads and their specific hardware demands can be supported. Additionally, heterogeneous hardware allows for a more granular employment of the required resources. For example a more cost saving ARM CPU could be utilized in low demand situations instead of a more potent but also energy demanding Intel CPU.

    \item [Resource Scalability]
    For isolated workloads the required computational power may be estimated accordingly to determine the appropriate resources. As soon as an infrastructure is provided to multiple users that can submit computations it becomes challenging to reach a stable utilization of the cluster. Thus providing static resources to a fluctuating amount of incoming computations likely leads to periods of underutilization or congestions. Therefore it is desirable to have the ability to dynamically increase the computational power of the cluster. One way to achieve this is the utilization of external cloud services like Microsoft Azure, Amazon EC2 etc.

    \item [Scalable Speed]
    While the general size of a cluster should be adjustable for overall resource utilization, the allocation of resources to a specific task also has to be considered. It is desirable that users are able to scale the execution speed of their respective workloads as real-world deadlines could put boundaries on the amount of available time for a computation to finish. Therefore it must be possible dynamically adjust resources for a workload without interrupting the overall execution. With that ability the framework would be enabled to handle burst scenarios as well as save resources in the case of continuous low computational demand by a computation.

    \item [Ease of Programming]
    The introduced framework should provide programmers with an easy to use distribution mechanism without adding noticeable overhead to their development process. Preferably the resulting code should be created in a modern high level language on a high abstraction level.

    \item [Workload Diversity]
    While the submitted workloads could differ in size and required resources they might also have different hardware requirements. For instance, several workloads within the system could be optimized for CPUs and others for GPUs. The framework should support the execution of these workload types and additionally optimize the distribution across available hardware from a global system view.

    \item [Optimized Scheduling]
    Aside from various hardware that workloads can be optimized for, running programs on a hybrid cloud adds the issue of networking distances between the local cluster and the external cloud. Thus, certain tasks may see performance improvements when run locally instead of being initially sent to external resources. It is desirable to take these factors into account and consider network distances and bottlenecks during scheduling decisions.

\end{description}
